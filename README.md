# üß© k8s-in-a-box

Kubernetes in a Box ‚Äì uma instala√ß√£o manual de um cluster Kubernetes com alta disponibilidade, provisionado via Ansible e orquestrado com Vagrant usando LibVirt.

![Kubernetes Dashboard](docs/cluster.png)

> üí° Constru√≠do usando o Kubernetes v1.35.0 ([v1.35 Timbernetes - The World Tree Release](https://kubernetes.io/blog/2025/12/17/kubernetes-v1-35-release/))

Este projeto nasceu como uma evolu√ß√£o natural de outro projeto de estudos ([vndmtrx/vagrant-k8s-cluster](https://github.com/vndmtrx/vagrant-k8s-cluster)), onde o cluster era criado utilizando o `kubeadm`. Durante aquele desenvolvimento, percebi que boa parte das etapas executadas pelo `kubeadm` (como a gera√ß√£o de certificados, configura√ß√£o do etcd e bootstrap dos componentes do control plane) aconteciam de forma autom√°tica, sem que eu realmente compreendesse o que estava acontecendo nos bastidores.

Com isso, o **k8s-in-a-box** surgiu como uma forma de reconstruir esse processo manualmente, etapa por etapa, para entender profundamente como o Kubernetes realmente se forma: dos certificados ao control plane e worker nodes.

Este projeto segue a filosofia *"Kubernetes The Hard Way"* ([kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)), demonstrando cada fase de instala√ß√£o dos componentes essenciais (`PKI`, `etcd`, `Control Plane` e `Worker Nodes`) sem recorrer a ferramentas de conveni√™ncia como `kubeadm` ou `k3s`.

O objetivo √© oferecer um laborat√≥rio de estudos que permita compreender os fundamentos do Kubernetes em sua forma mais pura, mantendo ainda a automa√ß√£o e reprodutibilidade via Ansible.

![Vagrant](https://img.shields.io/badge/Vagrant-1563FF?logo=vagrant&logoColor=white)
![Ansible](https://img.shields.io/badge/Ansible-EE0000?logo=ansible&logoColor=white)
![AlmaLinux](https://img.shields.io/badge/AlmaLinux-2D4F8C?logo=almalinux&logoColor=white)
![Kubernetes](https://img.shields.io/badge/Kubernetes-326CE5?logo=kubernetes&logoColor=white)
![etcd](https://img.shields.io/badge/etcd-419EDA?logo=etcd&logoColor=white)
![Nginx](https://img.shields.io/badge/Nginx-009639?logo=nginx&logoColor=white)
![Helm](https://img.shields.io/badge/Helm-277A9F?logo=helm&logoColor=white)

## Estado Atual do Projeto

Este projeto est√° em desenvolvimento ativo, seguindo uma abordagem progressiva de constru√ß√£o do cluster Kubernetes.  

Al√©m da implementa√ß√£o pr√°tica do cluster, tamb√©m est√° sendo criada uma documenta√ß√£o detalhada sobre cada etapa do processo, explicando as decis√µes tomadas em cada fase: da escolha de tecnologias e configura√ß√µes de rede/infraestrutura √† instala√ß√£o dos componentes de controle e n√≥s de trabalho.  

A implementa√ß√£o segue uma sequ√™ncia l√≥gica que respeita as depend√™ncias entre os componentes, garantindo reprodutibilidade e clareza em todo o processo.

### Documenta√ß√£o

Todo o material de refer√™ncia e guias de instala√ß√£o encontra‚Äëse no diret√≥rio `docs/` do reposit√≥rio. O √≠ndice completo, com links para cada se√ß√£o, est√° dispon√≠vel em [`docs/README.md`](./docs/README.md).

## Vis√£o Geral

O reposit√≥rio automatiza a cria√ß√£o de v√°rias m√°quinas virtuais em uma rede privada para as VMs, onde o cluster e as ferramentas anexas s√£o instaladas, n√£o criando nada na m√°quina host.

Com o Ansible como provedor de automa√ß√£o, cada componente do cluster √© instalado e configurado explicitamente: gera√ß√£o de certificados, cria√ß√£o do cluster `etcd`, deployment dos bin√°rios `kube‚Äëapiserver`, `controller‚Äëmanager`, `scheduler`, `containerd`, `kubelet` e `kube‚Äëproxy`, al√©m da instala√ß√£o dos v√°rios plugins de suporte do cluster.

### Componentes Conclu√≠dos
- Infraestrutura com Vagrant/LibVirt
- Framework de Automa√ß√£o Ansible
- Sistema Base das VMs (AlmaLinux 10)
- PKI (Certificados para todos componentes)
- Load Balancer (HAProxy)
- Cluster etcd
- Manager nodes (Control Plane)
- Worker Nodes
- Arquivos de Configura√ß√£o
- Addons de Cluster (CNI, CoreDNS, M√©tricas, Ingress Controller, Dashboard, MetalLB)
- Ferramentas de gerenciamento (etcdctl, kubectl, helm)
- Exemplos de deploys no cluster

> üí° Para um acompanhamento detalhado do desenvolvimento, incluindo todos os componentes e suas depend√™ncias, consulte o [Mapa de Progresso](docs/progresso.md).

## Decis√µes de Design

Algumas escolhas foram tomadas para simplificar o laborat√≥rio e maximizar o aprendizado:
1. **LibVirt + Vagrant**: optou‚Äëse pelo provider LibVirt devido ao desempenho superior e melhor integra√ß√£o com o Vagrant. Outros providers podem ser utilizados, mas n√£o est√£o cobertos neste projeto.
1. **Rede Privada 172.24.0.0/24**: todas as VMs est√£o em uma rede privada, evitando conflitos com redes dom√©sticas. Os pods e servi√ßos usam redes separadas para manter isolamento.
1. **Alta Disponibilidade**: HAProxy e Keepalived fornecem um VIP (`172.24.0.10`) e fazem balanceamento do etcd e do API Server, permitindo um failover transparente dos endpoints.
1. **Sistema Base AlmaLinux 10**: escolhido pela facilidade em rela√ß√£o √† configura√ß√£o de rede e pela disponibilidade de imagens atualizadas no Vagrant Cloud Images; outras distribui√ß√µes podem exigir ajustes.
1. **Certificados Gerenciados**: a gera√ß√£o de uma cadeia PKI completa (Root CA, CAs intermedi√°rias e certificados de cliente e servidor) garante seguran√ßa entre todos os componentes, e tamb√©m foi feita dessa forma para experimenta√ß√µes com rota√ß√£o de certificados.
1. **Runtime de Conteiners**: Foi utilizado o CRI-O pela simplicidade de instala√ß√£o na distribui√ß√£o atual. O containerd tamb√©m foi disponibilizado caso haja prefer√™ncia ou para estudo.
1. **Plugin de Rede**: Foi utilizado o Canal (Calico + Flannel) como padr√£o para uso completo dos recursos de rede, como Network Policies. O CNI Flannel simples tamb√©m foi disponibilizado.

## Arquitetura e Configura√ß√µes de Instala√ß√£o

Aqui est√° a base do laborat√≥rio: uma topologia m√≠nima funcional e uma topologia de refer√™ncia. A m√≠nima existe para quem tem menos mem√≥ria dispon√≠vel; a de refer√™ncia √© a em uso atualmente e serve de guia para as configura√ß√µes abaixo.

A personaliza√ß√£o do cluster √© feita em dois arquivos principais:
- `inventario/hosts.yml` que define as VMs que comp√µem o cluster. Cada host cont√©m endere√ßo IP, FQDN, mem√≥ria e CPU. M√°quinas adicionais podem ser habilitadas descomentando blocos adicionais para managers ou workers. Exemplos de defini√ß√µes padr√£o:
  - **Configura√ß√£o m√≠nima para rodar:**
    - 1x `LoadBalancer`: *vCPUs: 1; RAM: 384MB*
    - 1x `Servidor NFS`: *vCPUs: 1; RAM: 384MB*
    - 1x `Manager`:      *vCPUs: 1; RAM: 2048MB*
    - 1x `Worker`:       *vCPUs: 1; RAM: 2048MB*
    - 1x `Bastion Host`: *vCPUs: 1; RAM: 384MB*
    
    *Totalizando 5,2GB de RAM e 5 vCPUs;*
  
  - **Configura√ß√£o de Refer√™ncia do projeto:**
    - 2x `LoadBalancer`: *vCPUs: 2; RAM: 512MB*
    - 1x `Servidor NFS`: *vCPUs: 2; RAM: 384MB*
    - 3x `Manager`:      *vCPUs: 3; RAM: 3072MB*
    - 2x `Worker`:       *vCPUs: 3; RAM: 3072MB*
    - 1x `Bastion Host`: *vCPUs: 1; RAM: 384MB*

    *Totalizando 16,2GB de RAM e 11 vCPUs;*

* `inventario/group_vars/all.yml` define as vari√°veis globais do projeto e centraliza as configura√ß√µes que controlam o comportamento das *roles* do Ansible. √â nele que se personaliza a instala√ß√£o e o funcionamento do cluster.
  Algumas das principais op√ß√µes que podem ser ajustadas:

  * **Runtime de Conteiner** permite a escolha entre `crio` ou `containerd` para a parte de conteiners.
  * **Plugin de CNI:** permite escolher entre `flannel` ou `canal` (Flannel + Calico) para a rede dos pods.
  * **Vers√µes dos componentes:** define quais vers√µes do Kubernetes, etcd, Helm e CNI Plugins ser√£o utilizadas.
  * **Redes do cluster:** configura os blocos de endere√ßamento das redes de *hosts*, *pods* e *services*.
  * **Faixas de IPs do MetalLB:** controla os intervalos dispon√≠veis para LoadBalancers e IPs fixos.
  * **Par√¢metros de HAProxy e Keepalived:** ajusta timeouts, portas e o IP virtual (VIP) usado para alta disponibilidade.
  * **Certificados e artefatos:** define estrutura de diret√≥rios e se os certificados ser√£o regenerados automaticamente.

> üí° Juntos, `hosts.yml` e `all.yml` formam o n√∫cleo de personaliza√ß√£o do projeto: o primeiro define onde o cluster ser√° executado, e o segundo define como ele ser√° configurado.
> O cluster pode ser expandido ou reduzido conforme a capacidade de mem√≥ria e processamento dispon√≠vel, bastando ajustar os par√¢metros definidos nos arquivos de invent√°rio.

### Topologia de rede

As VMs ficam em uma **rede privada** (`172.24.0.0/24`) e os **pods/servi√ßos** usam faixas separadas para evitar conflitos:

* **Hosts (VMs):** `172.24.0.0/24`
* **Pods:** `172.25.0.0/17`
* **Servi√ßos:** `172.25.128.0/17`

O ambiente exp√µe um **VIP** para alta disponibilidade do plano de controle via Keepalived e HAProxy (`172.24.0.10`), mapeado por FQDNs como `api.k8sbox.local` e `etcd.k8sbox.local`.
Se desejar expor servi√ßos via LoadBalancer, h√° faixas pr√©-definidas para o MetalLB (pool ‚Äúmanuais‚Äù e pool ‚ÄúL2‚Äù), que podem ser ajustadas conforme sua rede local.

Inclusive, √© poss√≠vel verificar o status do HAProxy em [http://172.24.0.21:9000/stats](http://172.24.0.21:9000/stats) (usu√°rio/senha: `admin` / `senha_muito_segura!`).

> üí° A senha da p√°gina de status HAProxy √© exclusiva para o laborat√≥rio. Caso queira mudar, existe uma vari√°vel em [inventario/group_vars/all.yml](inventario/group_vars/all.yml) em que voc√™ pode alterar essa (e outras) informa√ß√µes.

### Componentes e alta disponibilidade

* **Balanceamento:** HAProxy faz o **failover** e o balanceamento do **kube-apiserver** e do **etcd**.
* **PKI:** toda a comunica√ß√£o entre componentes √© protegida por certificados emitidos pela **cadeia PKI** do projeto (Root CA + CAs intermedi√°rios para cada componente core).
* **Runtime:** `crio` como padr√£o pela simplicidade e estabilidade; `containerd` dispon√≠vel.
* **CNI:** `canal` (Calico + Flannel) como padr√£o (rede e pol√≠ticas); `flannel` dispon√≠vel como op√ß√£o mais leve.
* **Bastion (kubox):** host com `kubectl`, `etcdctl`, `helm` e utilit√°rios para operar e inspecionar o cluster sem ‚Äúpoluir‚Äù os n√≥s.

### Ordem de provisionamento (resumo)

O `Makefile` e os playbooks do Ansible conduzem a instala√ß√£o em etapas, respeitando as depend√™ncias:

1. **Artefatos e PKI** (bin√°rios, CAs e certificados)
2. **Sistema Base** (pr√©-requisitos de SO, tun√°veis de rede)
3. **Balanceador** (HAProxy/Keepalived)
4. **etcd** (cluster e mTLS)
5. **Control Plane** (API Server, Controller Manager, Scheduler)
6. **Workers** (containerd, kubelet, kube-proxy)
7. **Addons** (CNI, CoreDNS, m√©tricas, dashboard, ingress, MetalLB, etc.)

Voc√™ pode executar tudo de ponta a ponta com `make k8s-in-a-box` ou chamar **targets**/tags individuais para depurar etapas espec√≠ficas.

### Customiza√ß√µes r√°pidas

As principais op√ß√µes ficam em `inventario/group_vars/all.yml`:

* **Rede dos hosts/pods/servi√ßos:** `rede_cidr_hosts`, `rede_cidr_pods`, `rede_cidr_services`
* **CNI:** `plugin_cni: "canal"` (op√ß√µes: `canal` ou `flannel`)
* **VIP/HAProxy/Keepalived:** `keepalived_vip_ip`, `vip_api_fqdn`, `vip_etcd_fqdn`, timeouts e credenciais do HAProxy
* **MetalLB:** `metallb_ips_manuais` e `metallb_ips_loadbalacing`
* **Vers√µes:** `versao_kubernetes`, `versao_etcd`, `versao_cni`, `versao_helm`

> üí° Dica: ajuste primeiro CPU/RAM no `inventario/hosts.yml`. Em seguida, valide **rede** e **VIP**. Por fim, escolha o **CNI** conforme o objetivo: `canal` (recursos avan√ßados) ou `flannel` (menor consumo de mem√≥ria).

## In√≠cio R√°pido

1. Clone o reposit√≥rio:
```bash
git clone https://github.com/vndmtrx/k8s-in-a-box.git
cd k8s-in-a-box
```

2. Fa√ßa o provisionamento da estrutura completa
```bash
make k8s-in-a-box
```

## Acessando as VMs

Use o comando abaixo para acessar individualmente cada m√°quina virtual do projeto:
```bash
vagrant ssh <nome da VM>
```

## Rede

Todas as m√°quinas est√£o em uma rede privada (`172.24.0.0/24`), sendo as importantes para o acesso ao cluster as seguintes:
- IP Flutuante do balanceador: `172.24.0.10`
- IP do Bastion Host: `172.24.0.254`

## Opera√ß√£o do Cluster

Para a opera√ß√£o do cluster (e melhor simula√ß√£o de um ambiente real), as ferramentas de intera√ß√£o com o `etcd` e o cluster foram instaladas em um outro host, chamado `kubox`. Caso queira verificar o cluster, seguem alguns comandos √∫teis (para acessar o bastion host, use `vagrant ssh kubox`):
- `etcdctl`
  - Lista de Membros do cluster: `etcdctl member list -w json | yq -P | tspin`
  - Sa√∫de dos endpoints: `etcdctl endpoint health -w json | yq -P | tspin`
  - Status do cluster: `etcdctl endpoint status -w json | yq -P | tspin`
  - Status do Raft: `etcdctl endpoint hashkv -w json | yq -P | tspin`
- `kubectl`
  - Listar todos os n√≥s: `kubectl get nodes -o wide`
  - Listar todos os pods: `kubectl get pods -A -o wide`
  - √öltimos eventos do cluster: `kubectl get events -A --sort-by=.metadata.creationTimestamp`

## Acesso ao Dashboard e Inspetor de Rede

No cluster o Kubernetes Dashboard foi ativado, permitindo a verifica√ß√£o dos diversos componentes do cluster.

Para acessar o Kubernetes Dashboard, √© s√≥ acessar a URL [https://172.24.0.101](https://172.24.0.101/), e quando for solicitado o token, √© s√≥ usar o seguinte comando no bastion host:

```bash
kubectl -n dashboard create token dashboard-admin
```

## Destruindo o ambiente

Quando terminar os testes, o cluster pode ser destru√≠do com o comando:

```bash
make destroy
```

E caso queira destruir o ambiente e tamb√©m excluir os tempor√°rios baixados:

```bash
make clean
```

## Notas Importantes

- A chave SSH gerada por esse Vagrant (`id_ed25519.pub`) √© apenas para exemplo/desenvolvimento
- **N√ÉO USE** esta chave em ambiente de produ√ß√£o
- Para produ√ß√£o, sempre gere e use suas pr√≥prias chaves SSH
- O `Makefile` do projeto utiliza uma configura√ß√£o espec√≠fica do Ansible localizada em `./ansible/.ansible.cfg`

## Conformidade e Valida√ß√£o

Como parte do estudo, o cluster foi tamb√©m submetido ao teste de conformidade oficial da CNCF, utilizando a ferramenta `sonobuoy`.  
Esse teste √© o mesmo utilizado para validar distribui√ß√µes Kubernetes certificadas, verificando a compatibilidade e o comportamento esperado dos componentes centrais do sistema.

A execu√ß√£o foi **bem-sucedida**, com todos os testes aplic√°veis aprovados, confirmando que o ambiente atende √†s especifica√ß√µes oficiais do Kubernetes.  
O processo avaliou desde o plano de controle at√© os n√≥s de trabalho, garantindo a integridade e o funcionamento coerente de todo o cluster.

Esse resultado representa o esfor√ßo de automa√ß√£o constru√≠do com **Ansible**, **Vagrant** e **LibVirt**, que permite n√£o apenas reproduzir o ambiente de forma consistente, mas tamb√©m estudar e compreender cada etapa da forma√ß√£o de um cluster Kubernetes completo e funcional.

## Nota Pessoal

Este reposit√≥rio √© resultado de um estudo cont√≠nuo sobre como montar um cluster Kubernetes manualmente. Ele n√£o √© recomendado para uso em produ√ß√£o, apesar de ser bastante resiliente.

Este projeto n√£o √© apenas uma implementa√ß√£o, mas um caminho de estudo estruturado para compreender cada aspecto do funcionamento do Kubernetes.

Sinta‚Äëse √† vontade para contribuir com sugest√µes, issues e pull requests.

## Licen√ßa

Este projeto est√° licenciado sob a [Licen√ßa MIT](LICENSE) - veja o arquivo LICENSE para detalhes.

---

‚ö†Ô∏è **Projeto em desenvolvimento**
